{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8056acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" Title: The King's Compassionate Queen\\n\\nOnce upon a time, in the kingdom of Serendil, King Elden was a mighty, fair, and just ruler. He was known throughout the land for his wisdom and kindness. By his side, Queen Eirlys, a woman of great beauty and grace, not only was the wife of the king, but also his trusted advisor and the heart of the kingdom.\\n\\nOne day, a plague spread across the kingdom, affecting both the royal family and common folk alike. The king fell mortally ill, leaving the kingdom in turmoil as Queen Eirlys took charge to ensure the well-being of her people.\\n\\nQueen Eirlys' boundless compassion shone like a beacon, inspiring hope in the hearts of her people. She worked tirelessly, taking care of the sick, providing food to the hungry, and organizing doctors and healers to treat the afflicted. Despite her own grief, her strength never wavered.\\n\\nDays turned into weeks, and Queen Eirlys' persistence paid off. Slowly but surely, the plague started to recede, and the people of Serendil rejoiced. King Elden was able to regain his strength and returned to the throne.\\n\\nWith tearful gratitude, the people thanked both their king and queen, but it was Queen Eirlys who truly earned their admiration. King Elden looked upon his queen with newfound respect and love. He decided then and there that he would always be guided by her wisdom.\\n\\nFrom that day forward, the relationship between King Elden and Queen Eirlys grew even stronger, and their joint rule brought prosperity to the kingdom of Serendil. The Story of Queen Eirlys became an inspiration for generations, teaching the importance of love, compassion, and hard work in times of trouble. Their names will forever be remembered as the epitome of cooperation, love, and unwavering devotion.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mistral-7b/\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short story about king and queen\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c25b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Delhi', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"olmOCR-7B-0225-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Answer with just city name. What i capital of india? \"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load ShareGPT data\n",
    "with open(\"//home/skamalj/dev/vllm/ShareGPT_V3_unfiltered_cleaned_split.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten and filter valid user-only prompts\n",
    "def extract_user_prompts(convo):\n",
    "    return [msg[\"value\"] for msg in convo[\"conversations\"] if msg[\"from\"] == \"human\"]\n",
    "\n",
    "# Sample 100 random prompts\n",
    "random_samples = random.sample(data, 25)\n",
    "\n",
    "# Prepare OpenAI client (local server)\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\")\n",
    "\n",
    "# Iterate over samples\n",
    "for idx, sample in enumerate(random_samples):\n",
    "    prompts = extract_user_prompts(sample)\n",
    "    if not prompts:\n",
    "        continue\n",
    "\n",
    "    # Take only the **first** user prompt per sample\n",
    "    first_prompt = prompts[0].strip()\n",
    "    print(f\"Processing sample #{idx + 1} with prompt: {first_prompt}\")\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"my-mistral-model\",\n",
    "            messages=[{\"role\": \"user\", \"content\": first_prompt}]\n",
    "        )\n",
    "\n",
    "        print(f\"Sample #{idx + 1}\")\n",
    "        print(\"Prompt:\", first_prompt)\n",
    "        print(\"Response:\", completion.choices[0].message.content.strip())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error on sample #{idx + 1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f560ac",
   "metadata": {},
   "source": [
    "> Download  data file from [here](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6aa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load ShareGPT data\n",
    "with open(\"/home/skamalj/dev/hfdata/ShareGPT_V3_unfiltered_cleaned_split.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten and filter valid user-only prompts\n",
    "def extract_user_prompts(convo):\n",
    "    return [msg[\"value\"] for msg in convo[\"conversations\"] if msg[\"from\"] == \"human\"]\n",
    "\n",
    "# Sample 100 random prompts\n",
    "random_samples = random.sample(data, 500)\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\")\n",
    "\n",
    "# Prepare request payloads\n",
    "requests = []\n",
    "for sample in random_samples:\n",
    "    prompts = extract_user_prompts(sample)\n",
    "    if prompts:\n",
    "        requests.append(prompts[0].strip())\n",
    "\n",
    "# Function to run a single request\n",
    "def run_request(idx, prompt):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"my-mistral-model\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return idx, prompt, completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return idx, prompt, f\"ERROR: {e}\"\n",
    "\n",
    "# Run in parallel to trigger batching in vLLM\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    futures = [executor.submit(run_request, idx, prompt) for idx, prompt in enumerate(requests)]\n",
    "    for future in as_completed(futures):\n",
    "        idx, prompt, response = future.result()\n",
    "        print(f\"Sample #{idx + 1}\")\n",
    "        print(\"Prompt:\", prompt)\n",
    "        print(\"Response:\", response)\n",
    "        print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
