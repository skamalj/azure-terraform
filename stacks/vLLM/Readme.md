# Benchmark Results of 7Bâ€“8B LLMs on T4 and A10 GPUs

This table summarizes the benchmarking of **Mistral-7B**, **Qwen2.5-7B-Instruct**, and **Meta-Llama-3-8B-Instruct** models across **T4** and **A10 GPUs**.  
The metrics include token throughput, first-token latency (TTFT), cache utilization, and remarks on failures or constraints.  

Key takeaways:
- **T4 GPUs** have limited support (no bfloat16, frequent load failures, lower throughput).  
- **A10 GPUs** achieve much higher throughput and cache utilization, with stable performance across larger sequence lengths.  
- Misconfigured parameters (like `dtype` or `max_model_len`) often cause failures.  

---

| Model name                | GPU | Pipeline Parallel | No. of seq | Max model Len | dtype   | Token Throughput (per sec) | TTFT (sec) | Cache Utilization % | Result  | Remark                                                                                                                                                  |
|----------------------------|-----|------------------|------------|---------------|---------|----------------------------|------------|----------------------|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------|
| mistral-7b                 | T4  | 1                |            | 4096          | Not Set |                            |            |                      | Fail    | Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute capability 7.5. Use `--dtype=half` instead.    |
| mistral-7b                 | T4  | 1                |            | 4096          | half    |                            |            |                      | Fail    | Model Load failure                                                                                                                                      |
| mistral-7b                 | T4  | 2                | 32         | 4096          | half    | 390                        | 2          |                      | Success |                                                                                                                                                         |
| Qwen2.5-7B-Instruct        | T4  | 2                |            | 4096          | Not set |                            |            |                      | Fail    |                                                                                                                                                         |
| Qwen2.5-7B-Instruct        | T4  | 2                | 32         | 4096          | half    | 325                        | 2          | 12                   | Success |                                                                                                                                                         |
| Qwen2.5-7B-Instruct        | T4  | 2                | 128        | 4096          | half    | 320                        | 2          | 12                   | Success |                                                                                                                                                         |
| Meta-Llama-3-8B-Instruct   | T4  | 2                | 32         | 8192          | half    | 320                        | 2          | 25                   | Success |                                                                                                                                                         |
| Meta-Llama-3-8B-Instruct   | T4  | 2                | 32         | 4096          | half    | 300                        | 3          | 15                   | Success |                                                                                                                                                         |
| Meta-Llama-3-8B-Instruct   | A10 | 1                | 64         | 8192          | None    | 700                        | 1          | 60                   | Success |                                                                                                                                                         |
| Meta-Llama-3-8B-Instruct   | A10 | 1                | 128        | 16384         | None    |                            |            |                      | Failed  | ValueError: User-specified max_model_len (16384) is greater than derived max_model_len (max_position_embeddings=8192 or model_max_length=None).          |
| Meta-Llama-3-8B-Instruct   | A10 | 1                | 128        | 8192          | None    | 700                        | 1          | 70                   | Success |                                                                                                                                                         |
| mistral-7b                 | A10 | 1                | 64         | 8192          | None    | 700                        | 0.5        | 50                   | Success |                                                                                                                                                         |
| Qwen2.5-7B-Instruct        | A10 | 1                | 64         | 8192          | None    | 600                        | 1          | 30                   | Success |                                                                                                                                                         |
