{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eadeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525dcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=293)\u001b[0m INFO 2025-07-10 02:53:07,413 controller 293 -- Draining proxy on node 'fb8180e7e373de244188a0116034811028ec39aa8f3fccf3b6e853bb'.\n",
      "\u001b[36m(ServeController pid=293)\u001b[0m INFO 2025-07-10 02:53:07,414 controller 293 -- Removing proxy on removed node 'fb8180e7e373de244188a0116034811028ec39aa8f3fccf3b6e853bb'.\n",
      "Log channel is reconnecting. Logs produced while the connection was down can be found on the head node of the cluster in `ray_client_server_[port].out`\n",
      "2025-07-10 15:23:10,238\tWARNING dataclient.py:403 -- Encountered connection issues in the data channel. Attempting to reconnect.\n"
     ]
    }
   ],
   "source": [
    "ray.init(\"ray://localhost:10001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd096eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from ray.serve.llm import LLMConfig, build_openai_app, LLMServer, LLMRouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_config = LLMConfig(\n",
    "    model_loading_config=dict(\n",
    "        model_id=\"mistral-7b2\",\n",
    "        model_source=\"/models/llms/mistral-7b/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db\",\n",
    "    ),\n",
    "    deployment_config=dict(\n",
    "        autoscaling_config=dict(\n",
    "            min_replicas=1, max_replicas=1,\n",
    "        )\n",
    "    ),\n",
    "    # Pass the desired accelerator type (e.g. A10G, L4, etc.)\n",
    "    accelerator_type=\"T4\",\n",
    "    # You can customize the engine arguments (e.g. vLLM engine kwargs)\n",
    "    engine_kwargs=dict(\n",
    "        tensor_parallel_size=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = LLMServer.as_deployment(lm_config.get_serve_options(name_prefix=\"vLLM2:\")).bind(lm_config)\n",
    "llm_app = LLMRouter.as_deployment().bind([deployment])\n",
    "serve.run(llm_app, blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327c8629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752147240.522755   57371 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (1.90.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/skamalj/.conda/envs/ray/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8056acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"asctime\": \"2025-07-10 17:20:50,940\", \"levelname\": \"INFO\", \"message\": \"HTTP Request: POST http://localhost:8000/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\", \"filename\": \"_client.py\", \"lineno\": 1025, \"timestamp_ns\": 1752148250940208724}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" Title: The Gift of Gratitude\\n\\nOnce upon a time, in the vast and prosperous kingdom of Serendell, King Eldrone and Queen Evangeline ruled with wisdom and kindness. The kingdom was rich in resources, and its people lived happily, cherishing their beloved monarchs.\\n\\nKing Eldrone was a man of valor, and Queen Evangeline was a woman of grace and compassion. They were admired and loved by all, and their generous hearts were known far and wide.\\n\\nOne day, a humble traveler arrived at the royal court. He was a weaver from a distant village, seeking the king and queen's help. He told them about a terrible drought that had struck his home, and the villagers were suffering.\\n\\nThe king and queen, moved by the traveler's plea, decided to help. King Eldrone dispatched soldiers to the drought-stricken village, equipped with tools and supplies to dig a well.\\n\\nWeeks later, the well was complete, and the village was able to water its crops once again. The people rejoiced, and their prayers of gratitude reached the heavens.\\n\\nHowever, the traveler, who had returned to Serendell to report the news, felt compelled to express his appreciation in a more personal way. He decided to create a beautiful tapestry, a masterpiece of his craft, as a token of his gratitude.\\n\\nHe spent days and nights weaving the tapestry, pouring his heart and soul into every thread. Finally, the tapestry was complete, a stunning work of art that captured the victory against the drought and the unity of the kingdom.\\n\\nThe traveler presented the tapestry to King Eldrone and Queen Evangeline. They were speechless, moved by the craftsman's dedication and the beauty of the piece. They hung the tapestry in the grand hall, where it would be admired for generations to come.\\n\\nFrom that day forward, the traveler stopped traveling, choosing instead to live in Serendell. Every day, he would look at the tapestry, reminding him of the kindness of King Eldrone and Queen Evangeline, and their unwavering commitment to their people.\\n\\nAnd so, the tapestry became a symbol of gratitude and love, a testament to the power of a simple act of kindness and the profound impact it can have. The king and queen lived out the rest of their days, ruling with wisdom and compassion, and their legacy lived on in the hearts of their people and the tapestry that would forever remind them of their generosity.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"my-mistral-model\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short story about king and queen\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
